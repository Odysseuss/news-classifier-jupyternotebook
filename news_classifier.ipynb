{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets==4.0.1\n",
      "  Downloading tensorflow_datasets-4.0.1-py3-none-any.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets==4.0.1) (20.3.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets==4.0.1) (3.15.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets==4.0.1) (2.25.1)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.1.2-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets==4.0.1) (1.19.5)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets==4.0.1) (1.15.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets==4.0.1) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets==4.0.1) (4.58.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets==4.0.1) (0.11.0)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.6-cp38-cp38-manylinux_2_24_x86_64.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 5.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from tensorflow_datasets==4.0.1) (0.3.3)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.29.0-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 6.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets==4.0.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets==4.0.1) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets==4.0.1) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets==4.0.1) (1.26.3)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: future, promise\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=a968d78b9ce23cefa218a2d6a55b915f02614ad1a94de498344e4943c4289e47\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=f6b2403bfef94cac03845cb67d06d495561202d7ba6f8cf4652423456abba826\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built future promise\n",
      "Installing collected packages: googleapis-common-protos, tensorflow-metadata, promise, importlib-resources, future, dm-tree, tensorflow-datasets\n",
      "Successfully installed dm-tree-0.1.6 future-0.18.2 googleapis-common-protos-1.53.0 importlib-resources-5.1.2 promise-2.3 tensorflow-datasets-4.0.1 tensorflow-metadata-0.29.0\n",
      "tensorflow==2.4.1\n",
      "tensorflow-datasets==4.0.1\n",
      "tensorflow-estimator==2.4.0\n",
      "tensorflow-metadata==0.29.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets==4.0.1\n",
    "!pip freeze | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset ag_news_subset/1.0.0 (download: 11.24 MiB, generated: 35.79 MiB, total: 47.03 MiB) to /home/jovyan/tensorflow_datasets/ag_news_subset/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644da96451ca4c6eae00741d521ebe3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3434470d99dc4d39b66039dc44cc044b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798316b396ee4090b716ec5c5cba0bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/jovyan/tensorflow_datasets/ag_news_subset/1.0.0.incomplete4TOS6P/ag_news_subset-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef39b40ae65847d8af39e3eca5aedca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/jovyan/tensorflow_datasets/ag_news_subset/1.0.0.incomplete4TOS6P/ag_news_subset-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecc491130474a06a5e8bf96d832a7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset ag_news_subset downloaded and prepared to /home/jovyan/tensorflow_datasets/ag_news_subset/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "test, train = tfds.load('ag_news_subset', split=['test', 'train'], shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = tfds.as_dataframe(train.take(-1))\n",
    "df_test = tfds.as_dataframe(test.take(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_titles = []\n",
    "train_labels = []\n",
    "for title in df_train['title']:\n",
    "    train_titles.append(str(title))\n",
    "    \n",
    "for label in tf.constant(df_train['label']):\n",
    "    train_labels.append(label.numpy())\n",
    "\n",
    "train_labels_final = np.asarray(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_titles = []\n",
    "test_labels = []\n",
    "for title in df_test['title']:\n",
    "    test_titles.append(str(title))\n",
    "\n",
    "for label in tf.constant(df_test['label']):\n",
    "    test_labels.append(label.numpy())\n",
    "    \n",
    "test_labels_final = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocab_size=10000\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_length = 100\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_titles)\n",
    "padded_train_sequences = pad_sequences(train_sequences, max_sequence_length, truncating='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_titles)\n",
    "padded_test_sequences = pad_sequences(test_sequences, max_sequence_length, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ANN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "embedding_dimension = 16\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(tf.keras.layers.Embedding(vocab_size, embedding_dimension,\n",
    "                          input_length=max_sequence_length))\n",
    "\n",
    "classifier.add(layers.GlobalAveragePooling1D())\n",
    "\n",
    "classifier.add(Dense(units = 64, activation='relu', \n",
    "                     kernel_initializer = 'uniform'))\n",
    "\n",
    "\n",
    "classifier.add(Dense(units = 32, activation='relu', \n",
    "                     kernel_initializer='uniform'))\n",
    "classifier.add(Dropout(rate = .3))\n",
    "\n",
    "classifier.add(Dense(units =32, activation='relu', \n",
    "                     kernel_initializer='uniform'))\n",
    "classifier.add(Dropout(rate = .3))\n",
    "\n",
    "classifier.add(Dense(units =32, activation='relu', \n",
    "                     kernel_initializer='uniform'))\n",
    "classifier.add(Dropout(rate = .2))\n",
    "\n",
    "classifier.add(Dense(units =32, activation='relu', \n",
    "                     kernel_initializer='uniform'))\n",
    "classifier.add(Dropout(rate = .2))\n",
    "\n",
    "\n",
    "# Output Layer. Four output nodes for our four classification types of news headlines.\n",
    "classifier.add(Dense(units = 4, activation='softmax', \n",
    "                     kernel_initializer='uniform'))\n",
    "\n",
    "classifier.compile(optimizer = 'rmsprop', loss ='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "terminateOnNanCallback = callbacks.TerminateOnNaN()\n",
    "earlyStopCallback = callbacks.EarlyStopping(monitor='val_acc', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2479 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2489 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2519 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2523 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2483 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2457 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2509 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2509 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2472 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2502 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2493 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2509 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2511 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2475 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3863 - accuracy: 0.2525 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3863 - accuracy: 0.2532 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2508 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2490 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2493 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3863 - accuracy: 0.2525 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2472 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2481 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3863 - accuracy: 0.2483 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2509 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2477 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2529 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.3863 - accuracy: 0.2489 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3863 - accuracy: 0.2509 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2483 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2486 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3863 - accuracy: 0.2518 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.3864 - accuracy: 0.2498 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.3864 - accuracy: 0.2488 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2512 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3863 - accuracy: 0.2510 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2504 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2503 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.3864 - accuracy: 0.2500 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 39/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.3864 - accuracy: 0.2509 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3863 - accuracy: 0.2496 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 41/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.3864 - accuracy: 0.2497 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 42/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3863 - accuracy: 0.2493 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2501 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.3864 - accuracy: 0.2478 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2485 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 46/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3863 - accuracy: 0.2470 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2517 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3863 - accuracy: 0.2511 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 49/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3863 - accuracy: 0.2525 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 50/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2508 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2493 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 52/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.3864 - accuracy: 0.2478 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 53/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.3864 - accuracy: 0.2485 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 54/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2503 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 55/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.3864 - accuracy: 0.2482 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 56/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3864 - accuracy: 0.2511 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2489 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 58/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2492 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 59/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2485 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 60/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2493 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 61/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2505 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 62/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2507 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 63/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2504 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 64/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2468 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 65/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2487 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 66/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2495 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 67/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2485 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 68/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2478 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 69/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2492 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 70/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2503 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 71/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2470 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 72/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2503 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 73/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2505 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 74/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2463 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 75/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2490 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 76/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2506 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 77/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2507 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 78/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2503 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 79/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2473 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 80/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2490 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 81/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2473 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 82/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2528 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 83/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2458 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 84/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2508 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 85/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2489 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 86/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2486 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 87/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2505 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 88/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2513 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 89/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2479 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 90/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2478 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 91/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2480 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 92/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2504 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 93/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2483 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 94/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2519 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 95/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2506 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 96/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3864 - accuracy: 0.2490 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 97/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3863 - accuracy: 0.2515 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 98/100\n",
      "1185/1875 [=================>............] - ETA: 1s - loss: 1.3864 - accuracy: 0.2473"
     ]
    }
   ],
   "source": [
    "# Fit the ANN to the training data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "history = classifier.fit(padded_train_sequences,\n",
    "                         train_labels_final,\n",
    "                         epochs=100,\n",
    "                         batch_size=64,\n",
    "                         validation_data = (padded_test_sequences, test_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 0s 910us/step - loss: 1.3652 - accuracy: 0.3186\n",
      "Model accuracy on test data = 0.3185526430606842 \n"
     ]
    }
   ],
   "source": [
    "# Validate the ANN\n",
    "scores = classifier.evaluate(padded_test_sequences, test_labels_final)\n",
    "print(\"Model accuracy on test data = {} \".format(scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_keras_api_names',\n",
       " '_keras_api_names_v1',\n",
       " 'char_level',\n",
       " 'document_count',\n",
       " 'filters',\n",
       " 'fit_on_sequences',\n",
       " 'fit_on_texts',\n",
       " 'get_config',\n",
       " 'index_docs',\n",
       " 'index_word',\n",
       " 'lower',\n",
       " 'num_words',\n",
       " 'oov_token',\n",
       " 'sequences_to_matrix',\n",
       " 'sequences_to_texts',\n",
       " 'sequences_to_texts_generator',\n",
       " 'split',\n",
       " 'texts_to_matrix',\n",
       " 'texts_to_sequences',\n",
       " 'texts_to_sequences_generator',\n",
       " 'to_json',\n",
       " 'word_counts',\n",
       " 'word_docs',\n",
       " 'word_index']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "\n",
    "# URLS of known topics\n",
    "business_news_url=\"https://news.google.com/news/rss/headlines/section/topic/BUSINESS?ned=us&hl=en&gl=US\"\n",
    "tech_news_url = \"https://news.google.com/news/rss/headlines/section/topic/TECHNOLOGY?ned=us&hl=en&gl=US\"\n",
    "science_news_url = \"https://news.google.com/news/rss/headlines/section/topic/SCIENCE?ned=us&hl=en&gl=US\"\n",
    "health_news_url = \"https://news.google.com/news/rss/headlines/section/topic/HEALTH?ned=us&hl=en&gl=US\"\n",
    "entertainment_news_url = \"https://news.google.com/news/rss/headlines/section/topic/ENTERTAINMENT?ned=us&hl=en&gl=US\"\n",
    "\n",
    "# A dictionary with known topics mapped to a url from which we can harvest current headlines\n",
    "topics_to_headlines_url_dict = {'Business': business_news_url, \n",
    "                                'Technology': tech_news_url,\n",
    "                                'Science': science_news_url,\n",
    "                                'Health': health_news_url,\n",
    "                                'Entertainment': entertainment_news_url}\n",
    "\n",
    "# A function to scrape current headlines and the google news category\n",
    "def get_headlines(url):\n",
    "    Client = urlopen(url)\n",
    "    xml_page = Client.read()\n",
    "    Client.close()\n",
    "    \n",
    "    soup_page = soup(xml_page, \"html.parser\")\n",
    "    items = soup_page.findAll(\"item\")\n",
    "    title = soup_page.find(\"title\")\n",
    "    headlines = []\n",
    "    categories = []\n",
    "    for item in items:\n",
    "        headlines.append(item.title.text)\n",
    "        categories.append(title.text.split(' ')[0])\n",
    "    \n",
    "    df_headlines = pd.DataFrame({\"Headline\" : headlines, \"Actual Category\": categories})\n",
    "    display(\"{} items in category {}\".format(df_headlines.shape[0], categories[0]))   \n",
    "    \n",
    "    return pd.DataFrame({\"Headline\" : headlines, \"Actual Category\": categories})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps=[('tokenizer', tokenizer), ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape current google news headlines\n",
    "# of all known categories into a dataframe\n",
    "# for prediction\n",
    "df_inference = pd.DataFrame()\n",
    "for key, value in topics_to_headlines_url_dict.items():\n",
    "    df_inference = df_inference.append(get_headlines(value), ignore_index=True)\n",
    "\n",
    "    \n",
    "# Cut out all except 20 of the science and technology category rows\n",
    "# to bring it more in line with other categories\n",
    "number_to_remove = df_inference.loc[df_inference['Actual Category'].isin(['Science', 'Technology'])].shape[0] - 20\n",
    "df_inference = df_inference[~df_inference['Headline'].isin(df_inference.loc[df_inference['Actual Category'].isin(['Science', 'Technology'])].sample(number_to_remove)['Headline'])].reset_index(drop=True)\n",
    "\n",
    "# A function to convert category names to\n",
    "# the model representations of the category names\n",
    "def convert_category_name(x):\n",
    "    if x == \"Business\":\n",
    "        return 'b'\n",
    "    elif x == \"Science\" or x == \"Technology\":\n",
    "        return 't'\n",
    "    elif x == \"Health\":\n",
    "        return 'm'\n",
    "    elif x == \"Entertainment\":\n",
    "        return 'e'\n",
    "    \n",
    "df_inference['Actual Category'] = df_inference['Actual Category'].apply(lambda x: convert_category_name(x))\n",
    "\n",
    "classifier\n",
    "# pipeline.predict(df_inference.Headline.values)\n",
    "# Add model predictions to the dataframe\n",
    "# df_inference['Predicted Category'] = category_encoder.inverse_transform([x.argmax() for x in pipeline.predict(df_inference.Headline.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files to store model configuration and weights\n",
    "# such that the model can be built back up from the files\n",
    "# rather than having to retrain.\n",
    "# Thanks to https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "config_filename = \"model_config_\" + timestr + \".json\"\n",
    "weights_filename = \"model_weights_\" + timestr + \".h5\"\n",
    "\n",
    "# Serialize model to JSON\n",
    "classifier_json = classifier.to_json()\n",
    "with open(config_filename, \"w\") as json_file:\n",
    "    json_file.write(classifier_json)\n",
    "\n",
    "# Serialize weights to HDF5\n",
    "classifier.save_weights(weights_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
