{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "datafile = Path(\"./newsCorpora.csv\")\n",
    "datazipfile = Path(\"./NewsAggregatorDataset.zip\")\n",
    "urlstring = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\"\n",
    "\n",
    "if not datafile.exists():\n",
    "    if not datazipfile.exists():\n",
    "        datazipfile,_ = urllib.request.urlretrieve(urlstring)\n",
    "        print(datazipfile)\n",
    "    with ZipFile(datazipfile, 'r') as zip:\n",
    "            zip.extractall()\n",
    "        \n",
    "\n",
    "\n",
    "dataset = pd.read_csv(datafile, sep='\\t', names=['TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick our feature set (X) and what we are trying to predict (y)\n",
    "X = dataset.TITLE\n",
    "y = dataset.CATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data randomly for validation and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vectorizer for the healines\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, sublinear_tf=True, stop_words='english')\n",
    "vectorizer.fit(dataset.TITLE)\n",
    "\n",
    "# Save the vectorizer for use in loading saved model\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function perform preprocessing on data\n",
    "def process_data(X, y, vectorizer):\n",
    "    y = pd.get_dummies(y)\n",
    "    X = vectorizer.transform(X)\n",
    "    return X, y\n",
    "\n",
    "# Process the training data\n",
    "X_train.to_csv('X_train.csv')\n",
    "y_train.to_csv('y_train.csv')\n",
    "X_train, y_train = process_data(X_train, y_train, vectorizer)\n",
    "\n",
    "# Process the validation data\n",
    "X_val.to_csv('X_val.csv')\n",
    "y_val.to_csv('y_val.csv')\n",
    "X_val, y_val = process_data(X_val, y_val, vectorizer)\n",
    "\n",
    "# Process the test data for validation\n",
    "X_test.to_csv('X_test.csv')\n",
    "y_test.to_csv('y_test.csv')\n",
    "X_test, y_test = process_data(X_test, y_test, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Artificial Neural Network Architecture</h1>\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz11.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mortiz\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Create the ANN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "# Input layer\n",
    "classifier.add(Dense(units = 64, activation='relu', \n",
    "                     kernel_initializer = 'uniform',\n",
    "                     input_shape = (54345,)))\n",
    "\n",
    "# Eight Hidden Layers\n",
    "classifier.add(Dense(units = 32, activation='relu', \n",
    "                     kernel_initializer='uniform'))\n",
    "classifier.add(Dropout(rate = .3))\n",
    "\n",
    "classifier.add(Dense(units =32, activation='relu', \n",
    "                     kernel_initializer='uniform'))\n",
    "classifier.add(Dropout(rate = .3))\n",
    "\n",
    "classifier.add(Dense(units =32, activation='relu', \n",
    "                     kernel_initializer='uniform'))\n",
    "classifier.add(Dropout(rate = .2))\n",
    "\n",
    "classifier.add(Dense(units =32, activation='relu', \n",
    "                     kernel_initializer='uniform'))\n",
    "classifier.add(Dropout(rate = .2))\n",
    "\n",
    "\n",
    "# Output Layer. Four output nodes for our four classification types of news headlines.\n",
    "classifier.add(Dense(units = 4, activation='softmax', \n",
    "                     kernel_initializer='uniform'))\n",
    "\n",
    "classifier.compile(optimizer = 'rmsprop', loss ='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mortiz\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "\n",
    "terminateOnNanCallback = callbacks.TerminateOnNaN()\n",
    "tbCallback = callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "earlyStopCallback = callbacks.EarlyStopping(monitor='val_acc', patience=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 380177 samples, validate on 2113 samples\n",
      "Epoch 1/100\n",
      "380177/380177 [==============================] - 220s 579us/step - loss: 0.5165 - acc: 0.8124 - val_loss: 0.2647 - val_acc: 0.9200\n",
      "Epoch 2/100\n",
      "380177/380177 [==============================] - 221s 580us/step - loss: 0.2538 - acc: 0.9259 - val_loss: 0.2382 - val_acc: 0.9309\n",
      "Epoch 3/100\n",
      "380177/380177 [==============================] - 222s 583us/step - loss: 0.2318 - acc: 0.9345 - val_loss: 0.2367 - val_acc: 0.9285\n",
      "Epoch 4/100\n",
      "380177/380177 [==============================] - 220s 579us/step - loss: 0.2266 - acc: 0.9375 - val_loss: 0.2281 - val_acc: 0.9323\n",
      "Epoch 5/100\n",
      "380177/380177 [==============================] - 223s 587us/step - loss: 0.2237 - acc: 0.9400 - val_loss: 0.2480 - val_acc: 0.9319\n",
      "Epoch 6/100\n",
      "380177/380177 [==============================] - 218s 574us/step - loss: 0.2225 - acc: 0.9419 - val_loss: 0.2282 - val_acc: 0.9304\n",
      "Epoch 7/100\n",
      "380177/380177 [==============================] - 217s 571us/step - loss: 0.2231 - acc: 0.9428 - val_loss: 0.2357 - val_acc: 0.9347\n",
      "Epoch 8/100\n",
      "380177/380177 [==============================] - 216s 568us/step - loss: 0.2258 - acc: 0.9438 - val_loss: 0.2490 - val_acc: 0.9337\n",
      "Epoch 9/100\n",
      "380177/380177 [==============================] - 221s 582us/step - loss: 0.2330 - acc: 0.9446 - val_loss: 0.2418 - val_acc: 0.9352\n",
      "Epoch 10/100\n",
      "380177/380177 [==============================] - 222s 584us/step - loss: 0.2389 - acc: 0.9452 - val_loss: 0.2520 - val_acc: 0.9328\n",
      "Epoch 11/100\n",
      "380177/380177 [==============================] - 219s 576us/step - loss: 0.2480 - acc: 0.9451 - val_loss: 0.2485 - val_acc: 0.9375\n",
      "Epoch 12/100\n",
      "380177/380177 [==============================] - 217s 570us/step - loss: 0.2548 - acc: 0.9462 - val_loss: 0.2821 - val_acc: 0.9337\n",
      "Epoch 13/100\n",
      "380177/380177 [==============================] - 218s 573us/step - loss: 0.2678 - acc: 0.9459 - val_loss: 0.2946 - val_acc: 0.9352\n",
      "Epoch 14/100\n",
      "380177/380177 [==============================] - 218s 574us/step - loss: 0.2736 - acc: 0.9456 - val_loss: 0.3087 - val_acc: 0.9418\n",
      "Epoch 15/100\n",
      "380177/380177 [==============================] - 219s 576us/step - loss: 0.2817 - acc: 0.9456 - val_loss: 0.3284 - val_acc: 0.9404\n",
      "Epoch 16/100\n",
      "380177/380177 [==============================] - 218s 572us/step - loss: 0.2875 - acc: 0.9451 - val_loss: 0.3119 - val_acc: 0.9432\n",
      "Epoch 17/100\n",
      "380177/380177 [==============================] - 218s 572us/step - loss: 0.2903 - acc: 0.9451 - val_loss: 0.3367 - val_acc: 0.9380\n",
      "Epoch 18/100\n",
      "380177/380177 [==============================] - 219s 576us/step - loss: 0.2971 - acc: 0.9451 - val_loss: 0.3320 - val_acc: 0.9389\n",
      "Epoch 19/100\n",
      "380177/380177 [==============================] - 217s 570us/step - loss: 0.3052 - acc: 0.9451 - val_loss: 0.3702 - val_acc: 0.9385\n"
     ]
    }
   ],
   "source": [
    "# Fit the ANN to the training data\n",
    "history = classifier.fit(X_train, y_train, validation_data = (X_val, y_val), \n",
    "                         callbacks = [tbCallback, terminateOnNanCallback, earlyStopCallback], \n",
    "                         epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40129/40129 [==============================] - 18s 454us/step\n",
      "Model accuracy on test data = 0.9422861272396521 \n"
     ]
    }
   ],
   "source": [
    "# Validate the ANN\n",
    "scores = classifier.evaluate(X_test, y_test)\n",
    "print(\"Model accuracy on test data = {} \".format(scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files to store model configuration and weights\n",
    "# such that the model can be built back up from the files\n",
    "# rather than having to retrain.\n",
    "# Thanks to https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "config_filename = \"model_config_\" + timestr + \".json\"\n",
    "weights_filename = \"model_weights_\" + timestr + \".h5\"\n",
    "\n",
    "# Serialize model to JSON\n",
    "classifier_json = classifier.to_json()\n",
    "with open(config_filename, \"w\") as json_file:\n",
    "    json_file.write(classifier_json)\n",
    "\n",
    "# Serialize weights to HDF5\n",
    "classifier.save_weights(weights_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
